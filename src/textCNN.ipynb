{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerefence:\n",
    "\n",
    "Dive into Deep Learning \n",
    "\n",
    "Chapter 10\n",
    "\n",
    "Section 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import data as gdata, loss as gloss, nn \n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.contrib import text\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(\"./word2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(data):\n",
    "    wordsets = (' '.join(data['text'])).split(' ')\n",
    "    counter = collections.Counter(wordsets)\n",
    "    return text.vocab.Vocabulary(counter, min_freq=5, reserved_tokens=['<pad>'])\n",
    "\n",
    "\n",
    "def preprocess_data(data,vocab):\n",
    "    max_l = 1000\n",
    "    \n",
    "    def pad(x):\n",
    "        return x[:max_l] if len(x) > max_l else x + [vocab.token_to_idx['<pad>']] * (max_l - len(x))\n",
    "    \n",
    "    features = nd.array([pad(vocab.to_indices(text.split())) for text in data['text']])\n",
    "    labels = nd.array(data['label'])\n",
    "    return features, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      2  2967 6758 339 2021 1854 3731 4109 3792 4149 15...\n",
       "1     11  4464 486 6352 5619 2465 4802 1452 3137 5778 54...\n",
       "2      3  7346 4068 5074 3747 5681 6093 1777 2226 7354 6...\n",
       "3      2  7159 948 4866 2109 5520 2490 211 3956 5520 549...\n",
       "4      3  3646 3055 3055 2490 4659 6065 3370 5814 2465 5..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../input/train_set.csv', sep='\\t', nrows=20000)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = get_vocab(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#d2l.download_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.iloc[:18000,:]\n",
    "test_data = train_df.iloc[18000:20000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mxnet.contrib.text.vocab.Vocabulary at 0x7fd156a881d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "    *preprocess_data(train_data, vocab)), batch_size, shuffle = True)\n",
    "test_iter = gdata.DataLoader(gdata.ArrayDataset(\n",
    "    *preprocess_data(test_data, vocab)), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab, embed_size, kernel_sizes, num_channels,\n",
    "                     **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(len(vocab), embed_size)\n",
    "        # 不参与训练的嵌入层\n",
    "        self.constant_embedding = nn.Embedding(len(vocab),embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Dense(14)\n",
    "        # 时序最大池化层没有权重， 所以可以共用一个实例\n",
    "        self.pool = nn.GlobalMaxPool1D()\n",
    "        self.convs = nn.Sequential() # 创建多个一维卷积层\n",
    "        for c,k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.add(nn.Conv1D(c,k,activation='relu'))\n",
    "            \n",
    "    def forward(self, inputs):\n",
    "        # 将两个形状是（批量大小，词数，词向量维度）的嵌入层的输出按词向量连接\n",
    "        embeddings = nd.concat(\n",
    "            self.embedding(inputs), self.constant_embedding(inputs),dim=2)\n",
    "        # 根据Conv1D要求的输入格式，将词向量维，即一维卷积层的通道维，变换到前一维\n",
    "        embeddings = embeddings.transpose((0,2,1))\n",
    "        # 对于每一个一维卷积层， 在时许最大池化后会得到一个形状为（批量大小，通道大小，1）的\n",
    "        # NDArray。使用flatten函数去掉最后一维，然后再通道维上连接\n",
    "        encoding = nd.concat(*[nd.flatten(\n",
    "            self.pool(conv(embeddings))) for conv in self.convs], dim=1)\n",
    "        # 应用dropout后使用全连接层得到输出\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建一个TextCNN实例。它有3个卷积层，它们的核宽分别为3、4、5，输出通道数均为100。\n",
    "embed_size, kernel_sizes, num_channels = 100, [3,4,5], [100,100,100]\n",
    "ctx = d2l.try_all_gpus()\n",
    "net = TextCNN(vocab, embed_size, kernel_sizes, num_channels)\n",
    "net.initialize(init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 加载预训练的词向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=100\n",
    "def transform(word):\n",
    "            try:\n",
    "                vec = model.wv.get_vector(word)\n",
    "            except KeyError:\n",
    "                vec = np.array([0]*dim)\n",
    "            return vec\n",
    "       # [model.wv.get_vector(word) for word in txt.split(' ')]\n",
    "\n",
    "def data_to_vec(data):\n",
    "    return np.array([transform(word) for word in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.embedding.weight.set_data(data_to_vec(vocab.idx_to_token))\n",
    "net.constant_embedding.weight.set_data(data_to_vec(vocab.idx_to_token))\n",
    "net.constant_embedding.collect_params().setattr('grad_req','null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [cpu(0)]\n",
      "epoch 1, loss 1.1995, train acc 0.645, test acc 0.850, time 86.0 sec\n",
      "epoch 2, loss 0.5736, train acc 0.827, test acc 0.883, time 86.0 sec\n",
      "epoch 3, loss 0.4147, train acc 0.876, test acc 0.899, time 83.9 sec\n",
      "epoch 4, loss 0.3094, train acc 0.907, test acc 0.911, time 86.6 sec\n",
      "epoch 5, loss 0.2480, train acc 0.926, test acc 0.908, time 82.7 sec\n",
      "epoch 6, loss 0.2055, train acc 0.940, test acc 0.914, time 82.4 sec\n",
      "epoch 7, loss 0.1680, train acc 0.950, test acc 0.916, time 84.7 sec\n",
      "epoch 8, loss 0.1370, train acc 0.959, test acc 0.918, time 88.2 sec\n",
      "epoch 9, loss 0.1092, train acc 0.969, test acc 0.922, time 90.1 sec\n",
      "epoch 10, loss 0.0930, train acc 0.974, test acc 0.923, time 85.6 sec\n"
     ]
    }
   ],
   "source": [
    "# 2. 训练模型\n",
    "lr, num_epochs = 0.001, 10\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train(train_iter, test_iter, net, loss, trainer, ctx, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_handle = open('../pkl/textcnn-net.pkl','wb')\n",
    "pickle.dump(net,net_handle)\n",
    "net_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, vocab, sentence):\n",
    "    def try_gpu():\n",
    "        \"\"\"If GPU is available, return mx.gpu(0); else return mx.cpu().\"\"\"\n",
    "        try:\n",
    "            ctx = mxnet.gpu()\n",
    "            _ = nd.array([0], ctx=ctx)\n",
    "        except mxnet.base.MXNetError:\n",
    "            ctx = mxnet.cpu()\n",
    "        return ctx\n",
    "\n",
    "    \"\"\"Predict the sentiment of a given sentence.\"\"\"\n",
    "    sentence = nd.array(vocab.to_indices(sentence), ctx=try_gpu())\n",
    "    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)\n",
    "    logging.info()\n",
    "    return  int(label.asscalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5399 3117 1070 4321 4568 2621 5466 3772 4516 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2491 4109 1757 7539 648 3695 3038 4490 23 7019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2673 5076 6835 2835 5948 5677 3247 4124 2465 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4562 4893 2210 4761 3659 1324 2595 5949 4583 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4269 7134 2614 1724 4464 1324 3370 3370 2106 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  5399 3117 1070 4321 4568 2621 5466 3772 4516 2...\n",
       "1  2491 4109 1757 7539 648 3695 3038 4490 23 7019...\n",
       "2  2673 5076 6835 2835 5948 5677 3247 4124 2465 5...\n",
       "3  4562 4893 2210 4761 3659 1324 2595 5949 4583 2...\n",
       "4  4269 7134 2614 1724 4464 1324 3370 3370 2106 2..."
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/test_a.csv', sep='\\t')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label=test_df['text'].apply(lambda txt:predict(net, vocab, txt.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('../input/test_a_pred.csv','w')\n",
    "f.write('\\n'.join(['label']+list(pred_label.values.astype('str'))))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
